################################################################
# Test Nick's PatentsView code - assignee
# https://github.com/PatentsView/PatentsView-Disambiguation/blob/dev/docs/docs/assignee.md
#
# chris zhang 7/14/2021
################################################################

import pandas as pd
pd.set_option('max_colwidth', 100)
pd.set_option('display.max_columns', 999)
pd.set_option('display.width', 200)

from pv.disambiguation.core import AssigneeNameMention

from pv.disambiguation.assignee.load_name_mentions import Loader

# configuration
import configparser
config = configparser.ConfigParser()
config.read(['config/database_config.ini', 'config/database_tables.ini', 'config/assignee/run_clustering.ini'])

loader = Loader.from_config(config)

mentions = loader.load('musi')

print(mentions[0].__dict__)

# Featurizing Data
from pv.disambiguation.assignee.model import AssigneeModel
encoding_model = AssigneeModel.from_config(config)

print(list(map(encoding_model.feature_list,lambda x: x.name)))

features = encoding_model.encode(mentions)
print(features)

# Clustring model
# we don't have a trained model, we have a hand tuned one on the above features
model = AssigneeModel.from_config(config)
# run the clustering
from grinch.agglom import Agglom
grinch = Agglom(model, features, num_points=len(mentions))
grinch.build_dendrogram_hac()
# TODO: AttributeError: 'EncodingModel' object has no attribute 'weight_for'
fc = grinch.flat_clustering(model.aux['threshold'])

############################
# Putting it all together
############################
# python -m pv.disambiguation.assignee.build_assignee_name_mentions_sql
# python -m pv.disambiguation.assignee.run_clustering # TODO: for small test database set chunk_size=1000 in run_clustering.ini

job0 = pd.read_pickle('exp_out/assignee/run_26/job-0.pkl')
job0 = pd.DataFrame.from_dict(job0, orient='index')
job0 = job0.reset_index()
job0['cluster_size'] = [len(x) for x in job0[0]]

# TODO: how to read job-0internals.pkl?
# import pickle
# with open('exp_out/assignee/run_26/job-0internals.pkl', 'rb') as f:
#     data = pickle.load(f)
# ji0 = pd.read_pickle('exp_out/assignee/run_26/job-0internals.pkl')
#
# ji0 = pickle.Unpickler('exp_out/assignee/run_26/job-0internals.pkl')

########################
# Combine assignee clustering output table with rawassignee.tsv (from PV web)
########################
import configparser
from pv.disambiguation.util.db import granted_table, pregranted_table

config = configparser.ConfigParser()
config.read(['config/database_config.ini', 'config/database_tables.ini', 'config/inventor/run_clustering.ini'])

# create a connection to the table
granted = granted_table(config)
cursor = granted.cursor()

# get df of disambiguation maaping
query = "SELECT * from disambiguation_testing_granted.temp2_assignee_disambiguation_mapping;"
cols = ['uuid', 'assignee_id'] # , 'version_indicator' removed for now
cursor.execute(query)
disamb_map = pd.DataFrame(cursor, columns=cols)

# get df of raw assignees
query = "SELECT * from disambiguation_testing_granted.rawassignee;"
cols = ['uuid', 'patent_id', 'assignee_id', 'rawlocation_id', 'type',
        'name_first', 'name_last', 'organization', 'sequence', 'version_indicator',
        'created_date', 'updated_date']
cursor.execute(query)
rawassignee = pd.DataFrame(cursor, columns=cols)

# remove assignee_id col that was generated by using other (entire) sample, will merge in assignee_id from disamb_map
#del rawassignee['assignee_id']
# merge in assginee_id from disamb_map
disamb = pd.merge(disamb_map[['uuid', 'assignee_id']], rawassignee, how='right', on='uuid', indicator=True)
# disamb.assignee_id_x is from disamb_map
# disamb.assignee_id_y is from rawassginee
disamb = disamb.rename(columns={'assignee_id_x':'assignee_id_test', 'assignee_id_y':'assignee_id_orig'})

# get cluster size of each assignee_id for each patent
sizes = disamb['assignee_id_test'].value_counts()
sizes = sizes.reset_index()
sizes.columns = ['assignee_id_test', 'cluster_size']
disamb = pd.merge(disamb, sizes, on='assignee_id_test', how='left')
disamb = disamb.sort_values(by=['cluster_size', 'organization'])
# export as a test file for future easy use
disamb.to_csv('./test_data/assignee_disamb_temp2_branchMain.csv', index=False)
print('n_assignees = %s' % len(disamb))
print('Total number of assignee clusters = %s' % len(set(disamb['assignee_id_test'])))

# So far doing some eyeball checking, found following example issues:
# Abbot Laboratories (size=25) - Abbot Laboratories, Inc. (size=1) - stopword/presence >>fixed by main
# ADOBE SYSTEMS INCORPORATED (size=23) - Adobe Systems Inc. (size=1) - stopword/abbreviation
# Amazon Technologies, Inc. (size=108) - Amazon.com, Inc. (size=2) - big names
# Apple Computer, Inc. (size=12) - APPLE INC. (size=150) - big names
# ASM IP Holding B.V. (size=2) - ASM IP Holdings B.V. (size=1) - stopword/minor diff >>fixed by main
# AT&T (many examples) - big names
# Basf Aktiengesellschaft (size=66) - BASF Aktiengesellshcaft (size=1) - stopword/min typo >>fixed by main

# Above bug (re Abbott, BASF) is fixed using main branch (bug caused by dev/docs code)
# to update assignee results: build name mentions sql>>run clustering>>finalize>>upload

# TODO: remove stopwords and run clustering and check diff/improvements
# TODO: fix big name issue - post cleaning step?
# TODO: fix typo  â€“ post cleaning of relative editing distance?
# Abbott Laboraties
# ABBOTT LABORATORIES

############################
# Read assignee clustering output from local (rather than upload to/download from SQL)
############################

def upload(granted_ids, pregranted_ids, config):
    pairs_pregranted = []
    pairs_granted = []
    with open(config['ASSIGNEE_UPLOAD']['input'], 'r') as fin:
        for line in fin:
            splt = line.strip().split('\t')
            if splt[0] in pregranted_ids:
                pairs_pregranted.append((pregranted_ids[splt[0]], splt[1]))
            elif splt[0] in granted_ids:
                pairs_granted.append((granted_ids[splt[0]], splt[1]))

import pandas as pd
import pickle

# Read in assignee clustering output TSV (patent-cluster map)
fp = './exp_out/assignee/run_26/disambiguation_debug.tsv'
assignee_cluster_map = pd.read_csv(fp, sep='\t')
assignee_cluster_map.columns=['patent_id', 'assignee_id']

# Read in patent-uuid map
fp = './data/assignee/uuid.pkl'
granted_uuids, pgranted_uuids = pickle.load(open(fp, 'rb'))

# Generate uuid in assignee_cluster_map
assignee_cluster_map['uuid'] = [granted_uuids[x] if x in granted_uuids else pgranted_uuids[x]
                                for x in assignee_cluster_map['patent_id']]
